{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 8265  Val rows: 2020\n",
      "Train positives: 1556  /  8265\n",
      "Validation report (threshold=0.5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9981    0.9600    0.9787      1648\n",
      "           1     0.8483    0.9919    0.9145       372\n",
      "\n",
      "    accuracy                         0.9658      2020\n",
      "   macro avg     0.9232    0.9759    0.9466      2020\n",
      "weighted avg     0.9705    0.9658    0.9668      2020\n",
      "\n",
      "Confusion matrix:\n",
      " [[1582   66]\n",
      " [   3  369]]\n",
      "ROC AUC: 0.9980474866896336\n",
      "PR AUC : 0.9917803481634222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import List, Optional, Iterable, Dict, Tuple\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from sklearn.svm import OneClassSVM\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# https://docs.python.org/3/library/dataclasses.html\n",
    "''' class ConfigBlock:\n",
    "decorator for fields defined by class variables\n",
    "    \"block_type\": strings extracted while parsing the config to delineate/group sub-lines of config sections, \n",
    "        like \"interface\" configs, think of it as a section identifier that groups all interface blocks\n",
    "    \"header\": string for a \"root\" unindented-line of indented-sublines from a config, like \"interface GigabitEthernet1/0/1\"\n",
    "    \"lines\": list of strings containing all lines from that block including the header\n",
    "'''\n",
    "@dataclass\n",
    "class ConfigBlock:\n",
    "    block_type: str\n",
    "    header: str\n",
    "    lines: List[str]\n",
    "    def text(self) -> str:\n",
    "        return '\\n'.join(self.lines)\n",
    "\n",
    "''' def NormalizeHashes()\n",
    "Regex substitution to replace hashed passwords, secrets; keys:\n",
    "    hashed strings are dense and subject to large variations between configurations.\n",
    "    Which ironically, the variations are greater when using cryptographically secure functions like:\n",
    "    `secret` which returns a type 9 hash, or `password encryption aes` globally, or `hmac-sha2-256` on keys.\n",
    "    These hashed output variations penalize scoring of secure configurations because -\n",
    "        using a reversable hash like type 7 results in the same or extremely similar hashed strings on all device's config.\n",
    "So, by normalizing any hashed string, it allows the classifier during training time to learn features representing the actual configured algorithm -\n",
    "    like noncompliant `sha1` or `md5` vs compliant algorithms.\n",
    "'''\n",
    "def NormalizeHashes(line: str) -> str:\n",
    "    s = line.rstrip('\\r\\n')\n",
    "    s = re.sub(r'^(ntp authentication-key\\s+\\d+\\s+\\S+)\\s+.*$', r'\\1 <HASH>', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'^(username\\s.*(?:secret|password)\\s+\\d+\\s+).+$', r'\\1<HASH>', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'^(enable\\s+(?:secret|password)\\s+\\d+\\s+).+$', r'\\1<HASH>', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'^(.*(server-key)\\s+).+$', r'\\1<HASH>', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'^(key\\s\\d{1})\\s.+$', r'\\1<HASH>', s, flags=re.IGNORECASE)    \n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "'''def NormalizeSSH()\n",
    "The config section for defining allowed SSH client & server algorithms is subject to a ton of variability.\n",
    "For example, the algorithms in: \"ip ssh server algorithm encryption aes256-gcm aes128-gcm aes256-ctr aes192-ctr aes128-ctr 3des-cbc\" -\n",
    "    can appear in any order, combination, and combination of orders.\n",
    "This makes it difficult to train a classifier to reliably detect the presence of a string like `3des-cbc`.\n",
    "Now apply this same problem to 8 lines of SSH server & client algorithm configs in total.\n",
    "However, after ordering the algorithms in training & testing, the true-positive rate went up and the false-positive rate went down.\n",
    "'''\n",
    "def NormalizeSSH(line: str) -> str:\n",
    "    s = re.sub(r'\\s+', ' ', line.strip())\n",
    "    low = s.lower()\n",
    "    m = re.match(r'^(ip ssh (server|client) algorithm (\\S+)) (.+)$', low)\n",
    "    if not m:\n",
    "        return s\n",
    "    prefix = m.group(1)\n",
    "    rest = m.group(4)\n",
    "    algos = rest.split()\n",
    "    algos_sorted = ' '.join(sorted(algos))\n",
    "    return prefix + ' ' + algos_sorted\n",
    "\n",
    "''' def FlagWeakSSH()\n",
    "To further improve the classifiers ability to consistently identify noncompliant SSH client/server configuration lines,\n",
    "FlagWeakSSH() examines each line of ssh algorithm against the WeakSSH set.\n",
    "Each line like \"ip ssh server algorithm encryption aes256-gcm aes128-gcm aes256-ctr aes192-ctr aes128-ctr 3des-cbc\"\n",
    "    is broken to its parts and those parts are tagged based on the regex match groups. The line above becomes:\n",
    "        platform  = server\n",
    "        field     = encryption\n",
    "        remainder = aes256-gcm aes128-gcm aes256-ctr aes192-ctr aes128-ctr 3des-cbc\n",
    "If a weak algorithm is found, OFFENDER_{algorithm} is appended to the tag list which looks like this:\n",
    "    [\"SSH_PLATFORM_server\", \"SSH_FIELD_encryption\", \"OFFENDER_3des-cbc\"]\n",
    "'''\n",
    "WeakSSH = {'hmac-sha1','3des-cbc','diffie-hellman-group14-sha1','ssh-rsa','x509v3-ssh-rsa'}\n",
    "def FlagWeakSSH(line: str) -> str:\n",
    "    s = re.sub(r'\\s+', ' ', line.strip())\n",
    "    low = s.lower()\n",
    "    m = re.match(r'^ip ssh (server|client) algorithm (\\S+) (.+)$', low)\n",
    "    if not m:\n",
    "        return s\n",
    "    platform, field, remainder = m.group(1), m.group(2), m.group(3)\n",
    "    algos = remainder.split()\n",
    "    tags = [\n",
    "        f'SSH_PLATFORM_{platform}',\n",
    "        f'SSH_FIELD_{field}',\n",
    "    ]\n",
    "    for a in algos:\n",
    "        if a in WeakSSH:\n",
    "            tags.append(f'OFFENDER_{a}')\n",
    "    return s + ' ' + ' '.join(tags)\n",
    "\n",
    "'''def LoadTrainData()\n",
    "Loads a single csv dataset for UNSUPERVISED training of: OCSVM, LOF, AE.\n",
    "The file contains 100 pseudo-random baseline \"compliant\" switch configs.\n",
    "Each unique running-config is delineated by the naturally occurring \"end\" which marks the last line of any IOS XE running-config.\n",
    "'''\n",
    "def LoadTrainData(path: str, line_col: str = 'line', end_token: str = 'end'):\n",
    "    df = pd.read_csv(path)\n",
    "    assert line_col in df.columns, f'Expected column \"{line_col}\" in {path}'\n",
    "    compliant_configs = []\n",
    "    current = []\n",
    "    for raw in df[line_col].astype(str).tolist():\n",
    "        line = raw.rstrip('\\r\\n')\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "        current.append(line)\n",
    "        if line.strip().lower() == end_token:\n",
    "            compliant_configs.append(current)\n",
    "            current = []\n",
    "    if current:\n",
    "        compliant_configs.append(current)\n",
    "    return compliant_configs\n",
    "\n",
    "'''def ExtractBlockHeader()\n",
    "Regex \"match\" or \"startswith\" method extracts specific headers for grouping sublines; otherwise categorizing lines with:\n",
    "    specific functionality/pertinence beyond simply \"GLOBAL\"\n",
    "This enables more granular detection of misconfigurations in these blocks of features; -v.s.-\n",
    "    attempting to learn that a compliant ntp key is \"hmac-sha2-256\" amongst all the other training/testing noise in the global configuration.\n",
    "This proved to be an effective and expedient solution to improve results in unsupervised learning while,\n",
    "    avoiding determinism or jumping to labeled & supervised training.\n",
    "'''\n",
    "def ExtractBlockHeader(line: str) -> Optional[str]:\n",
    "    line = line.strip()\n",
    "    if re.match(r'^(ip http|no ip http)', line):\n",
    "        return 'http'\n",
    "    if re.match(r'^interface Vlan', line):\n",
    "        return 'interface Vlan'\n",
    "    if re.match(r'^interface [^V]', line):\n",
    "        return 'interface'\n",
    "    if re.match(r'^snmp-server [^e]', line):\n",
    "        return 'snmp-server'\n",
    "    if line.startswith('line con'):\n",
    "        return 'line con'\n",
    "    if line.startswith('ip ssh source'):\n",
    "        return 'ip ssh source'\n",
    "    if line.startswith('ip ssh client '):\n",
    "        return 'ip ssh client'\n",
    "    if line.startswith('ip ssh server '):\n",
    "        return 'ip ssh server'\n",
    "    if line.startswith('line vty'):\n",
    "        return 'line vty'\n",
    "    if line.startswith('spann'):\n",
    "        return 'spanning-tree'\n",
    "    if line.startswith('login'):\n",
    "        return 'login'\n",
    "    if line.startswith('vtp'):\n",
    "        return 'vtp'\n",
    "    if line.startswith('ntp'):\n",
    "        return 'ntp'\n",
    "    if line.startswith('enable'):\n",
    "        return 'enable'\n",
    "    if line.startswith('username'):\n",
    "        return 'username'\n",
    "    if line.startswith('version'):\n",
    "        return 'version'\n",
    "    if re.match(r'logging', line):\n",
    "        return 'logging'\n",
    "    if re.match(r'aaa group', line):\n",
    "        return 'aaa group'\n",
    "    if re.match(r'aaa authen', line):\n",
    "        return 'aaa authentication'\n",
    "    if re.match(r'aaa authori', line):\n",
    "        return 'aaa authorization'\n",
    "    if re.match(r'aaa accou', line):\n",
    "        return 'aaa accounting'\n",
    "    if re.match(r'aaa common', line):\n",
    "        return 'aaa common-criteria'\n",
    "    if re.match(r'aaa server', line):\n",
    "        return 'aaa server'\n",
    "    if re.match(r'^vlan\\s+\\S+', line):\n",
    "        return 'vlan'\n",
    "    if line.startswith('ip access-list standard '):\n",
    "        return 'ip access-list standard'\n",
    "    if line.startswith('ip access-list extended '):\n",
    "        return 'ip access-list extended'        \n",
    "    if line.startswith('tacacs server '):\n",
    "        return 'tacacs server'\n",
    "    if line.startswith('radius server '):\n",
    "        return 'radius server'\n",
    "    if line.startswith('ip default'):\n",
    "        return 'ip default'\n",
    "    if re.match(r'service', line):\n",
    "        return 'service'\n",
    "    if line.startswith('class-map '):\n",
    "        return 'class-map'\n",
    "    if line.startswith('policy-map '):\n",
    "        return 'policy-map'\n",
    "    if line.startswith('netconf-yang'):\n",
    "        return 'netconf-yang'\n",
    "    if line.startswith('call-home'):\n",
    "        return 'call-home'\n",
    "    return None\n",
    "\n",
    "'''ExtractConfigBlocks()\n",
    "Create blocks of configuration lines based on the indentation and on the header\n",
    "So, lines within an interface configuration is blocked as\n",
    "    \"block_type\" = \"interface\"\n",
    "    \"header\" = \"interface GigabitEthernet1/0/1\"\n",
    "    \"lines\" = all subsequent, indented lines\n",
    "The block header is discovered through the \"ExtractBlockHeader\" function above. \n",
    "If a line does not match, ExtractBlockHeader returns None, and subsequently - \"ExtractConfigBlocks\" sets the \"block_type\" and \"header\" as global.\n",
    "'''\n",
    "def ExtractConfigBlocks(ConfigLines: Iterable[str]) -> List[ConfigBlock]:\n",
    "    blocks: List[ConfigBlock] = []\n",
    "    CurrentBlock: Optional[ConfigBlock] = None\n",
    "    global_lines: List[str] = []\n",
    "    for raw_line in ConfigLines:\n",
    "        line = raw_line.rstrip('\\n\\r')\n",
    "        stripped = line.strip()\n",
    "        if stripped == '' or stripped == '!' or stripped.lower() == 'end':\n",
    "            continue\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        if indent == 0:\n",
    "            block_type = ExtractBlockHeader(stripped)\n",
    "            if block_type is not None:\n",
    "                if CurrentBlock is not None:\n",
    "                    blocks.append(CurrentBlock)\n",
    "                    CurrentBlock = None\n",
    "                CurrentBlock = ConfigBlock(block_type=block_type,header=stripped,lines=[stripped])\n",
    "            else:\n",
    "                if CurrentBlock is not None:\n",
    "                    blocks.append(CurrentBlock)\n",
    "                    CurrentBlock = None\n",
    "                global_lines.append(stripped)\n",
    "        else:\n",
    "            if CurrentBlock is not None:\n",
    "                CurrentBlock.lines.append(stripped)\n",
    "            else:\n",
    "                global_lines.append(stripped)\n",
    "    if CurrentBlock is not None:\n",
    "        blocks.append(CurrentBlock)\n",
    "    if global_lines:\n",
    "        blocks.append(ConfigBlock(\n",
    "            block_type='global',\n",
    "            header='GLOBAL-CONFIG',\n",
    "            lines=['GLOBAL-CONFIG'] + global_lines))\n",
    "    return blocks\n",
    "\n",
    "'''LoadTestData()\n",
    "opens each CSV test-set-config file, blocks the lines, sanatizes the lines, converts doc to DataFrame of the blocked & sanatized line rows\n",
    "'''\n",
    "def LoadTestData(test_dir: str, pattern: str = '*.csv', line_col: str = 'line') -> pd.DataFrame:\n",
    "    rows = []\n",
    "    files = sorted(glob.glob(os.path.join(test_dir, pattern)))\n",
    "    if not files:\n",
    "        return pd.DataFrame(columns=['__hostname','__source_file','__block_idx','block_type','header','block_text','lines_list'])\n",
    "    for f in files:\n",
    "        base = os.path.basename(f)\n",
    "        hostname = os.path.splitext(base)[0]\n",
    "        df1 = pd.read_csv(f)\n",
    "        if line_col not in df1.columns:\n",
    "            raise ValueError(f'{f} missing required column \"{line_col}\"')\n",
    "        test_lines = []\n",
    "        for raw in df1[line_col].astype(str).tolist():\n",
    "            ln = raw.rstrip('\\r\\n')\n",
    "            if ln.strip() == '':\n",
    "                continue\n",
    "            test_lines.append(ln)\n",
    "        blocks = ExtractConfigBlocks(test_lines)\n",
    "        for bi, b in enumerate(blocks):\n",
    "            rows.append({'__hostname': hostname,'__source_file': base,'__block_idx': bi,'block_type': b.block_type,'header': b.header,\n",
    "                 'block_text': BlockToDoc(b),'lines_list': b.lines,})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "'''BlockToDoc()\n",
    "Normalizes lines within a block, and creates a document of blocks distinguished by \"BLOCKTYPE_\" and deliniated by \\n\n",
    "'''\n",
    "def BlockToDoc(block: ConfigBlock) -> str:\n",
    "    out = []\n",
    "    for ln in block.lines:\n",
    "        ln2 = NormalizeHashes(ln)\n",
    "        ln2 = NormalizeSSH(ln2)\n",
    "        ln2 = FlagWeakSSH(ln2)\n",
    "        out.append(ln2)\n",
    "    return f'BLOCKTYPE_{block.block_type}\\n' + '\\n'.join(out)\n",
    "\n",
    "'''SupervisedNormalization()\n",
    "Really similar to \"BlockToDoc\" but called in \"FeatureText\" and \"BlocksToLinesDF\"\n",
    "'''\n",
    "def SupervisedNormalization(line: str) -> str:\n",
    "    s = str(line).rstrip('\\r\\n')\n",
    "    s = NormalizeHashes(s)\n",
    "    s = NormalizeSSH(s)\n",
    "    s = FlagWeakSSH(s)\n",
    "    return s    \n",
    "\n",
    "def FeatureText(scope: str, parent: str, line: str) -> str:\n",
    "    scope = (scope or 'UNKNOWN').strip()\n",
    "    parent = (parent or 'GLOBAL').strip()\n",
    "    line_n = SupervisedNormalization(line)\n",
    "    parent_tok = parent.lower()\n",
    "    parent_tok = re.sub(r'[^a-z0-9]+', '_', parent_tok).strip('_')\n",
    "    return f\"SCOPE_{scope} PARENT_{parent_tok} {line_n}\"    \n",
    "\n",
    "'''BlocksToLinesDF()\n",
    "This function is for supervised classification test time with the imbalanced learn model.\n",
    "Because imbalanced learn was trained (fit) on the labeled training data that has \"scope\", \"parent\", \"lines\" in its shape - \n",
    "the unlabled test datasets lines need to be reshaped based on the \"class ConfigBlock\" fields:\n",
    "    \"scope\" inherits \"block_type\"\n",
    "    \"parent\" inherits \"header\"\n",
    "    \"lines\" inherit \"lines_list\"\n",
    "This is doubly necessary because the goal is to provide LINE LEVEL detection in the supervised model,\n",
    "    whereas, in unsupervised, we detected and reported on BLOCK LEVELs because line level isn't possible in the first place.\n",
    "But, I want to avoid the file-ops of re-reading all test-set config CSVs and building another DataFrame, etc..\n",
    "    So, this function reuses the \"TestData_BlocksDF\" from unsupervised learning - \n",
    "    converts the DF from BLOCK to LINE level with the necessary lables to give each line the correct feature shape for imbalanced learn.\n",
    "'''\n",
    "def BlocksToLinesDF(TestData_BlocksDF: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    required = [\"__hostname\", \"block_type\", \"header\", \"lines_list\"]\n",
    "    for c in required:\n",
    "        if c not in TestData_BlocksDF.columns:\n",
    "            raise ValueError(f\"TestData_BlocksDF missing column: {c}\")\n",
    "    for _, r in TestData_BlocksDF.iterrows():\n",
    "        hostname = r[\"__hostname\"]\n",
    "        scope = str(r[\"block_type\"])\n",
    "        parent = str(r[\"header\"])\n",
    "        lines = r[\"lines_list\"]\n",
    "        #if lines_list is nan or string, norm it\n",
    "        if lines is None or (isinstance(lines, float) and np.isnan(lines)):\n",
    "            continue\n",
    "        #if stored as a string of a list\n",
    "        if isinstance(lines, str):\n",
    "            #treat as single line rather than character sequence\n",
    "            lines = [lines]\n",
    "        if not isinstance(lines, (list, tuple)):\n",
    "            lines = [str(lines)]\n",
    "        for ln in lines:\n",
    "            #strip and normalize lines \n",
    "            s = SupervisedNormalization(ln)\n",
    "            if s == \"\" or s == \"!\" or s.lower() == \"end\":\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"__hostname\": hostname,\n",
    "                \"scope\": scope,\n",
    "                \"parent\": parent,\n",
    "                \"line\": s,\n",
    "            })\n",
    "    out = pd.DataFrame(rows)\n",
    "    return out\n",
    "    \n",
    "''' def ResultWriter()\n",
    "set \"FlaggedOnly\" = \"True\" to write out only configuration lines that were classified as noncompliant\n",
    "set \"topN\" = an integer as upper bound for number of lines by decending score to write out\n",
    "will make the directory from \"OutDir\", then joins the os.path with the filename that is \"Results_{hostname}.xlsx\"\n",
    "writes that hostname's results to xlsx, opens the xlsx file; examines each rows flag columns for log, ocsvm; ae:\n",
    "    if a row has no -1 flag set in any model's flag column, then the row remains un-filled\n",
    "    if a row has a -1 flag set in AE or LOF flag column, then the row is filled orange\n",
    "    if a row has a -1 flag set in AE+LOF or (AE+OCSVM|LOF+OCSVM) flag columns, then the row is filled red\n",
    "    if a row has a -1 flag set in all three model's flag columns, then the row is filled purple\n",
    "'''\n",
    "def ResultWriter(ScoredBlocksDF: pd.DataFrame, dfSuperBlocks: pd.DataFrame, OutDir: str, FlaggedOnly: bool = False, topN: Optional[int] = None):\n",
    "    os.makedirs(OutDir, exist_ok=True)\n",
    "    #pregroup supervised results by hostname\n",
    "    super_by_host = {h: g.copy() for h, g in dfSuperBlocks.groupby('__hostname', sort=True)}\n",
    "    for hostname, d1 in ScoredBlocksDF.groupby('__hostname', sort=True):\n",
    "        d1 = d1.sort_values(['__block_idx'])\n",
    "        if FlaggedOnly and 'any_flag' in d1.columns:\n",
    "            d1 = d1[d1['any_flag'] == 1]\n",
    "        if topN is not None and 'max_score' in d1.columns:\n",
    "            top = d1.sort_values('max_score', ascending=False).head(topN)\n",
    "            d1 = top.sort_values('__block_idx')\n",
    "        #supervised df for this same hostname\n",
    "        d2 = super_by_host.get(hostname, pd.DataFrame())\n",
    "        WriteDF1 = d1.drop(columns=['lines_list'], errors='ignore')\n",
    "        WriteDF2 = d2\n",
    "        OutPath = os.path.join(OutDir, f'Results_{hostname}.xlsx')\n",
    "        #one write; two sheets\n",
    "        with pd.ExcelWriter(OutPath, engine='openpyxl') as writer:\n",
    "            WriteDF1.to_excel(writer, sheet_name='LOF-OCSVM-AE', index=False)\n",
    "            WriteDF2.to_excel(writer, sheet_name='Imbalance-Learn', index=False)\n",
    "        #openpyxl format fill color on unsupervised res\n",
    "        wb = load_workbook(OutPath)\n",
    "        ws = wb['LOF-OCSVM-AE'] #name the worksheet for unsupervised model results\n",
    "        OrangeFill = PatternFill(start_color='F79646', end_color='F79646', fill_type='solid')\n",
    "        RedFill    = PatternFill(start_color='FF5050', end_color='FF5050', fill_type='solid')\n",
    "        PurpleFill = PatternFill(start_color='9966FF', end_color='9966FF', fill_type='solid')\n",
    "        header_row = 1\n",
    "        headers = {cell.value: idx + 1 for idx, cell in enumerate(ws[header_row])}\n",
    "        lof_col   = headers.get('lof_flag')\n",
    "        ocsvm_col = headers.get('ocsvm_flag')\n",
    "        ae_col    = headers.get('ae_flags') or headers.get('ae_flag')\n",
    "        if lof_col and ocsvm_col and ae_col:\n",
    "            for row_idx in range(2, ws.max_row + 1):\n",
    "                lof_flag   = ws.cell(row=row_idx, column=lof_col).value\n",
    "                ocsvm_flag = ws.cell(row=row_idx, column=ocsvm_col).value\n",
    "                ae_flag    = ws.cell(row=row_idx, column=ae_col).value\n",
    "                \n",
    "                #OCSVM is the most prone to false-positives, while not contributing unique strengths/results to true-positives,\n",
    "                #so I hastily \"weighted\" results in the color-coding:\n",
    "                #All 3 models is filled purple, LOF + AE or OCSVM + either LOF or AE is filled red, LOF or AE alone is filled orange.\n",
    "                if lof_flag == -1 and ocsvm_flag == -1 and ae_flag == -1:\n",
    "                    fill = PurpleFill\n",
    "                elif (lof_flag == -1 and ocsvm_flag == -1) or (ocsvm_flag == -1 and ae_flag == -1) or (lof_flag == -1 and ae_flag == -1):\n",
    "                    fill = RedFill\n",
    "                elif lof_flag == -1 or ae_flag == -1:\n",
    "                    fill = OrangeFill\n",
    "                else:\n",
    "                    continue\n",
    "                for col_idx in range(1, ws.max_column + 1):\n",
    "                    ws.cell(row=row_idx, column=col_idx).fill = fill\n",
    "        wb.save(OutPath)\n",
    "        wb.close()\n",
    "\n",
    "SVD_DIM = 100\n",
    "svd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#--- TRAINING DATA ---\n",
    "'''\n",
    "Load into TrainDocs: <-- blocked <-- normalized <-- \n",
    "'''\n",
    "CompliantConfigs_TrainData = LoadTrainData('CompliantSwitchDataSet.csv', line_col='line', end_token='end')\n",
    "TrainBlocks: List[ConfigBlock] = []\n",
    "for ConfigLines in CompliantConfigs_TrainData:\n",
    "    blocks = ExtractConfigBlocks(ConfigLines)\n",
    "    TrainBlocks.extend(blocks)\n",
    "TrainDocs = [BlockToDoc(b) for b in TrainBlocks]\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "''' Term Frequency Inverse-Document Frequency:\n",
    "Convert text into vectors; calculate TF of terms in each block of \"TrainDocs\"; calculate IDF across all blocks. --> X_BlocksTrain\n",
    "`token_pattern` to match any word char, forward slash or hyphen and, extracts uni/bigrams; with default min_df of 1\n",
    "'''\n",
    "BlockVectorize = TfidfVectorizer(token_pattern=r'[\\w/-]+',ngram_range=(1, 2),min_df=1)\n",
    "X_BlocksTrain = BlockVectorize.fit_transform(TrainDocs)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "''' Singular Value Decomposition \"svd\":\n",
    "Calls scikit-learn TruncatedSVD with n_components=100 and random_state=42\n",
    "Linear dimensionality reduction, efficient on sparse matrices; such as tf-idf for latent semantic analysis (LSA).\n",
    "n_components sets output data dimensionality; 100 is the recommended value for LSA.\n",
    "'''\n",
    "X_DenseTrain = svd.fit_transform(X_BlocksTrain)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "''' StandardScalar:\n",
    "Center & scale independently on each feature by calculating on each sample in training set; mean/std-dev used later in .transform\n",
    "Important in SVM where features are assumed to center around 0 with variance in the same order.\n",
    "'''\n",
    "X_DenseTrain = scaler.fit_transform(X_DenseTrain)\n",
    "\n",
    "#--- TEST DATA ---\n",
    "'''\n",
    "Load into DataFrame (TestData_BlocksDF): <-- blocked <-- normalized <-- config lines <-- Test Datasets (random noncompliant switch config CSVs) \n",
    "Used at test time for unsupervised (LOF, OCSVM; AE) and supervised (Imbalance Learn) with modifcations addressed later.\n",
    "Basically, these are the real world switch configs that you would run the tool against. A mix of compliant and noncompliant.\n",
    "'''\n",
    "TestData_ConfigDir = r'C:\\Users\\PhilipMcDowell\\00.01_PurdueLocal\\573\\Project\\TestConfigs'\n",
    "#TEST DATA: extraction --> blocks --> DataFrame --> \n",
    "TestData_BlocksDF = LoadTestData(TestData_ConfigDir, line_col='line')\n",
    "# docs --> tfIDF --> SVD --> StdScaler \n",
    "TestDocs = TestData_BlocksDF['block_text'].tolist()\n",
    "X_BlocksTestSet = BlockVectorize.transform(TestDocs) #vectorize blocks from test doc\n",
    "X_DenseTest  = svd.transform(X_BlocksTestSet) #RECALL: svd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\n",
    "X_DenseTest  = scaler.transform(X_DenseTest) #RECALL: scaler = StandardScaler()\n",
    "\n",
    "BlocksTestSet = [ConfigBlock(block_type=row['block_type'], header=row['header'], lines=row['lines_list'])for _, row in TestData_BlocksDF.iterrows()]\n",
    "\n",
    "#--- UNSUPERVISED ---\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n",
    "# https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection\n",
    "''' Local Outlier Factor: \n",
    "Unsupervised novelty detection; computes local density deviation of data point wrt its neighbors.\n",
    "Should typically set \"n_neighbors \" > min # samples that a cluster may contain & < max number of close samples that could be local outliers...\n",
    "Or said another way: \"n_neighbors=20 appears to work well in general\"; it is the actual number of neighbors used for kneighbor queries.\n",
    "Critically, \"novelty\" default setting is False because LOF is meant to be used for outlier detection.SortedResultsOCSVM\n",
    "to use LOF for novelty; set \"=True\" -- and only use `predict`, `decision_function` and `score_samples` on new unseen data\n",
    "'''\n",
    "lof = LocalOutlierFactor(n_neighbors=35, novelty=True, metric='euclidean')\n",
    "lof.fit(X_DenseTrain)\n",
    "lofDecision = lof.decision_function(X_DenseTest)\n",
    "NoveltyScoresLOF = -lofDecision\n",
    "lof_flag = lof.predict(X_DenseTest)\n",
    "lofResultsBlock = list(zip(BlocksTestSet, NoveltyScoresLOF, lof_flag))\n",
    "lofSortedResultsBlock = sorted(lofResultsBlock, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n",
    "# https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection\n",
    "''' One Class SVM (OCSVM): \n",
    "Support Vector Machines would traditionally be used in supervised learning with labeled datasets allowing multiple classes.\n",
    "But, as the name implies, One Class SVM is designed to perform novlety detection of datasets with one class, unsupervised.\n",
    "It creates a learned \"frontier\" based on the high-dimensional distibution of training data.\n",
    "New, test data that falls beyond that distribution frontier is novel.\n",
    "'''\n",
    "ocsvm = OneClassSVM(kernel='rbf', nu=0.05, gamma='scale')\n",
    "ocsvm.fit(X_BlocksTrain)\n",
    "DecisionScores = ocsvm.decision_function(X_BlocksTestSet)\n",
    "ocsvmNoveltyScores = -DecisionScores\n",
    "ocsvmFlags = ocsvm.predict(X_BlocksTestSet)\n",
    "ocsvm_block_results = list(zip(BlocksTestSet, ocsvmNoveltyScores, ocsvmFlags))\n",
    "ocsvm_block_results_sorted = sorted(ocsvm_block_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "''' reconstruction Autoencoder:\n",
    "AE is an effective comparison against OCSVM and LOF as a \"self-supervised\" model, \n",
    "    without building some fully connected Large Language Neural Network monster.\n",
    "    \n",
    "High-level: AE's take input, encode the input to a compressed form, then decode the compressed input to a reconstructed version of the original input.\n",
    "Oversimplified, if an AE accurately reconstructs input test data, then it has been trained on that (or mathematically very similar) data...\n",
    "    So, if a an AE absolutely bombs reconstruction of test data, then that input is data the AE was not trained on.\n",
    "\n",
    "This is where the Mean Squared Error comes in. We train AE on the vector-space of the text data from varied compliant configs.\n",
    "When it encodes, decodes, reconstructs test data: the reconstructed input with a low MSE is likely compliant;\n",
    "    and, reconstructed input with a high MSE is novel (and in our case, likely noncompliant).\n",
    "\n",
    "Chunking the configs into associated blocks and normalizing the lines in the blocks is *highly beneficial* to AE.\n",
    "'''\n",
    "AE_SVD_DIM = 400\n",
    "AE_H1 = 128\n",
    "AE_BOTTLENECK = 64\n",
    "AE_LR = 1e-3\n",
    "AE_EPOCHS = 40\n",
    "AE_BATCH = 128\n",
    "AE_VAL_SPLIT = 0.10\n",
    "AE_SEED = 42\n",
    "AE_THRESH_Q = 0.98\n",
    "def _set_seeds(seed: int = AE_SEED):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "def svdScaleFitTransform(X_sparse, svd_dim: int) -> Tuple[TruncatedSVD, StandardScaler, np.ndarray]:\n",
    "    max_dim = max(2, min(svd_dim, X_sparse.shape[0] - 1, X_sparse.shape[1] - 1))\n",
    "    svd = TruncatedSVD(n_components=max_dim, random_state=AE_SEED)\n",
    "    X_dense = svd.fit_transform(X_sparse)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_dense)\n",
    "    return svd, scaler, X_scaled\n",
    "\n",
    "def svdScaleTransform(X_sparse, svd: TruncatedSVD, scaler: StandardScaler) -> np.ndarray:\n",
    "    X_dense = svd.transform(X_sparse)\n",
    "    X_scaled = scaler.transform(X_dense)\n",
    "    return X_scaled\n",
    "\n",
    "def BuildAE(input_dim: int, h1: int = AE_H1, bottleneck: int = AE_BOTTLENECK) -> keras.Model:\n",
    "    model = keras.Sequential([layers.Input(shape=(input_dim,)),layers.Dense(h1, activation='relu'),\n",
    "        layers.Dense(bottleneck, activation='relu'),layers.Dense(h1, activation='relu'),layers.Dense(input_dim, activation='linear')])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(AE_LR), loss='mse')\n",
    "    return model\n",
    "\n",
    "def ReconstructionMSE(x_true: np.ndarray, x_pred: np.ndarray) -> np.ndarray:\n",
    "    return np.mean((x_true - x_pred) ** 2, axis=1)\n",
    "\n",
    "@dataclass\n",
    "class PipeAE:\n",
    "    svd: TruncatedSVD\n",
    "    scaler: StandardScaler\n",
    "    ae: keras.Model\n",
    "    threshold: float\n",
    "\n",
    "def FitAE(X_train_blocks,svd_dim: int = AE_SVD_DIM,thresh_q: float = AE_THRESH_Q) -> PipeAE:\n",
    "    _set_seeds(AE_SEED)\n",
    "    svd, scaler, X_train = svdScaleFitTransform(X_train_blocks, svd_dim=svd_dim)\n",
    "    ae = BuildAE(input_dim=X_train.shape[1], h1=AE_H1, bottleneck=AE_BOTTLENECK)\n",
    "    ae.fit(X_train, X_train,epochs=AE_EPOCHS,batch_size=AE_BATCH,validation_split=AE_VAL_SPLIT,shuffle=True,verbose=0)\n",
    "    X_train_recon = ae.predict(X_train, verbose=0)\n",
    "    train_err = ReconstructionMSE(X_train, X_train_recon)\n",
    "    T = float(np.quantile(train_err, thresh_q))\n",
    "    return PipeAE(svd=svd, scaler=scaler, ae=ae, threshold=T)\n",
    "\n",
    "def ScorePipe(pipe: PipeAE, X_test_sparse) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_test = svdScaleTransform(X_test_sparse, pipe.svd, pipe.scaler)\n",
    "    X_recon = pipe.ae.predict(X_test, verbose=0)\n",
    "    err = ReconstructionMSE(X_test, X_recon)\n",
    "    flag = np.where(err >= pipe.threshold, -1, 1)\n",
    "    return err, flag\n",
    "\n",
    "ae_pipe = FitAE(X_BlocksTrain, svd_dim=AE_SVD_DIM, thresh_q=AE_THRESH_Q)\n",
    "\n",
    "ae_scores, ae_flags = ScorePipe(ae_pipe, X_BlocksTestSet)\n",
    "\n",
    "BlockResultsAE = list(zip(BlocksTestSet, ae_scores, ae_flags))\n",
    "\n",
    "ScoredBlocksDF = TestData_BlocksDF.copy()\n",
    "ScoredBlocksDF['lof_score'] = NoveltyScoresLOF.astype(float)\n",
    "ScoredBlocksDF['lof_flag']  = lof_flag.astype(int)\n",
    "ScoredBlocksDF['ocsvm_score'] = ocsvmNoveltyScores.astype(float)\n",
    "ScoredBlocksDF['ocsvm_flag']  = ocsvmFlags.astype(int)\n",
    "ScoredBlocksDF['ae_scores'] = ae_scores.astype(float)\n",
    "ScoredBlocksDF['ae_flags']  = ae_flags.astype(int)\n",
    "ScoredBlocksDF['any_flag'] = (\n",
    "    (ScoredBlocksDF['lof_flag'] == -1) | (ScoredBlocksDF['ocsvm_flag'] == -1) | (ScoredBlocksDF['ae_flags'] == -1)).astype(int)\n",
    "\n",
    "ScoredBlocksDF['max_score'] = ScoredBlocksDF[['lof_score', 'ocsvm_score', 'ae_scores']].max(axis=1)\n",
    "\n",
    "# https://imbalanced-learn.org/stable/user_guide.html#user-guide\n",
    "''' imbalanced learn:\n",
    "IMB learn is a method to address the problem of training/testing in data where the ratio of classes are highly imbalanced.\n",
    "For example, switch configurations where, there's typically many more compliant lines than noncompliant lines.\n",
    "This imbalance can impact the classifiers ability to learn class differences and identify the different classes at test time.\n",
    "\n",
    "For this implementation of IMB learn, the labeled training dataset must have the following columns:\n",
    "    \"config_id\", \"label\", \"scope\", \"parent\", \"line\"\n",
    "'''\n",
    "IGNORE_LABEL_2 = True\n",
    "SCOPE_FILTER = None\n",
    "RANDOM_SEED = 42\n",
    "TrainDF = pd.read_csv('LabeledSwitchDataSet.csv')\n",
    "\n",
    "if SCOPE_FILTER is not None:\n",
    "    TrainDF = TrainDF[TrainDF['scope'].astype(str).str.lower() == SCOPE_FILTER.lower()].copy()\n",
    "\n",
    "#convert label\n",
    "TrainDF['label'] = pd.to_numeric(TrainDF['label'], errors='coerce')\n",
    "\n",
    "#drop irrelevant label 2\n",
    "if IGNORE_LABEL_2:\n",
    "    TrainDF = TrainDF[TrainDF['label'].isin([0, 1])].copy()\n",
    "\n",
    "#binary target\n",
    "y = TrainDF['label'].astype(int).values\n",
    "\n",
    "#feature text\n",
    "X_Text = [FeatureText(scope, parent, line) for scope, parent, line in zip(TrainDF['scope'], TrainDF['parent'], TrainDF['line'])]\n",
    "\n",
    "groups = TrainDF['config_id'].astype(str).values  # so train/val doesn't leak within same config\n",
    "\n",
    "#per config_id train/validation split to prevent leakage\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=RANDOM_SEED)\n",
    "TrainIDX, ValIDX = next(gss.split(X_Text, y, groups=groups))\n",
    "\n",
    "X_TrainText = [X_Text[i] for i in TrainIDX]\n",
    "y_Train = y[TrainIDX]\n",
    "\n",
    "X_ValText = [X_Text[i] for i in ValIDX]\n",
    "y_Val = y[ValIDX]\n",
    "\n",
    "print('Train rows:', len(X_TrainText), ' Val rows:', len(X_ValText))\n",
    "print('Train positives:', int(np.sum(y_Train == 1)), ' / ', len(y_Train))\n",
    "\n",
    "#char_wb ngrams: robust to small changes, or ordering; quirks, etc\n",
    "vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), min_df=1)\n",
    "\n",
    "#oversample minority class, label=1, to reduce bias\n",
    "ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "\n",
    "#LR on sparse TF-IDF\n",
    "clf = LogisticRegression(max_iter=3000, solver='liblinear')\n",
    "\n",
    "imb = ImbPipeline(steps=[('vec', vec), ('ros', ros), ('clf', clf)])\n",
    "imb.fit(X_TrainText, y_Train)\n",
    "\n",
    "#eval held-out configs val split\n",
    "ValPr = imb.predict_proba(X_ValText)[:, 1]\n",
    "ValPred = (ValPr >= 0.5).astype(int)\n",
    "\n",
    "print('Validation report (threshold=0.5)')\n",
    "print(classification_report(y_Val, ValPred, digits=4))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_Val, ValPred))\n",
    "try:\n",
    "    print('ROC AUC:', roc_auc_score(y_Val, ValPr))\n",
    "    print('PR AUC :', average_precision_score(y_Val, ValPr))\n",
    "except Exception as e:\n",
    "    print('AUC computation skipped:', e)\n",
    "\n",
    "TestDF = BlocksToLinesDF(TestData_BlocksDF)\n",
    "X_TestText = [FeatureText(scope, parent, line) for scope, parent, line in zip(TestDF['scope'], TestDF['parent'], TestDF['line'])]\n",
    "\n",
    "TestPr = imb.predict_proba(X_TestText)[:, 1]\n",
    "TestPred = (TestPr >= 0.5).astype(int)\n",
    "\n",
    "TestDF['pred_label'] = TestPred\n",
    "TestDF['prob_noncompliant'] = TestPr\n",
    "\n",
    "SupervisedResultsDF = TestDF.copy()\n",
    "SupervisedResultsDF['pred_label'] = TestPred\n",
    "SupervisedResultsDF['prob_noncompliant'] = TestPr\n",
    "\n",
    "SortedSupervisedResultsDF = SupervisedResultsDF.sort_values('prob_noncompliant', ascending=False)\n",
    "# ''' uncomment for a quick view of imbalanced learn results '''\n",
    "# print(SortedSupervisedResultsDF.head(30).to_string(index=False))\n",
    "\n",
    "ResultsDir = r'C:\\Users\\PhilipMcDowell\\00.01_PurdueLocal\\573\\Project\\Results'\n",
    "ResultWriter(ScoredBlocksDF, SortedSupervisedResultsDF, ResultsDir, FlaggedOnly=False, topN=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
